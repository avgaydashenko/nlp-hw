{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "from summa import summarizer\n",
    "from gensim.summarization import summarize\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "from alphabet_detector import AlphabetDetector\n",
    "ad = AlphabetDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_43428_1.txt') as file_data:\n",
    "    result = []\n",
    "    for text in file_data:\n",
    "        if text in ['[\\n', ']\\n']:\n",
    "            continue\n",
    "        else:\n",
    "            orig_text = '. '.join([' '.join(words) for words in split(text)])\n",
    "            if len(orig_text) <= 300:\n",
    "                result.append('\"' + orig_text + '\"')\n",
    "                continue\n",
    "            text = summarize(orig_text, word_count=300)\n",
    "#             text += ' ' + orig_text\n",
    "            text = text.replace('\\n', ' ')\n",
    "            text = text.replace('.', ' ')\n",
    "            text = text.replace('  ', ' ')\n",
    "\n",
    "            result.append('\"' + text[:300] + '\"')\n",
    "#             break\n",
    "            \n",
    "with open('output.txt', 'w') as file_res:\n",
    "    file_res.write(\"[\" + ', '.join(result) + \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(text):\n",
    "    tmp = []\n",
    "    curr_sent = []\n",
    "    curr_word = ''\n",
    "    for ch in text:\n",
    "        curr_word = curr_word.lower()\n",
    "        if ch == ' ':\n",
    "            if len(curr_word) > 0:\n",
    "                if ad.only_alphabet_chars(curr_word, \"CYRILLIC\"):\n",
    "                    curr_sent.append(curr_word)\n",
    "                curr_word = ''\n",
    "        elif ch.isalpha():\n",
    "            if ch == 'n':\n",
    "                if len(curr_word) > 1:\n",
    "                    if ad.only_alphabet_chars(curr_word, \"CYRILLIC\"):\n",
    "#                         print(curr_word)\n",
    "                        curr_sent.append(curr_word)\n",
    "                    curr_word = ''\n",
    "                if len(curr_sent) > 1:\n",
    "                    tmp.append(curr_sent)\n",
    "                    curr_sent = []\n",
    "            else:\n",
    "                if ad.only_alphabet_chars(curr_word, \"CYRILLIC\"):\n",
    "                    curr_word += ch\n",
    "\n",
    "    if len(curr_word) > 0:\n",
    "        if ad.only_alphabet_chars(curr_word, \"CYRILLIC\"):\n",
    "            curr_sent.append(curr_word)\n",
    "    if len(curr_sent) > 0:\n",
    "        tmp.append(curr_sent)\n",
    "    \n",
    "#     tmp = tmp[:3] + tmp[-1]\n",
    "    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annot(text):        \n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    for i in range(1, min(4, len(text))):\n",
    "        result += ' '.join(text[i]) + ' '\n",
    "    \n",
    "    result += ' '.join(text[-1]) + ' '\n",
    "        \n",
    "    return result[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annot(text):        \n",
    "    words = {}\n",
    "\n",
    "    for sent in text:\n",
    "        for word in sent:\n",
    "            word = word.lower()\n",
    "            words[word] = words.get(word, 0) + 1\n",
    "            \n",
    "#     print(words)\n",
    "            \n",
    "    most_freq = max(list(words.values()))\n",
    "    \n",
    "    scores = np.zeros(len(text))\n",
    "\n",
    "    for i, sent in enumerate(text):\n",
    "        for j, word in enumerate(sent):\n",
    "            word = word.lower()\n",
    "            next_freq = None if j == len(sent) - 1 else words[sent[j + 1].lower()]\n",
    "            if words[word] not in [1, most_freq]:\n",
    "                scores[i] += words[word] * (1 if next_freq is None or next_freq == most_freq else next_freq)\n",
    "        scores[i] /= len(sent)\n",
    "        \n",
    "    ind = np.argsort(scores)\n",
    "    \n",
    "    result = ''\n",
    "    for i in ind:\n",
    "        result += ' '.join(text[i]) + ' '\n",
    "        \n",
    "    return result[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
